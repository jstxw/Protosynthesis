[
  {
    "id": "instruction_add_node",
    "text": "How to add a node to your workflow canvas. Step 1: Locate the Node Palette on the left side of the screen. You can open it by clicking the plus icon or pressing 'N' on your keyboard. Step 2: Browse available nodes organized by category (AI, Communication, Payment, Productivity, Database, Utility). Alternatively, use the search bar at the top of the palette to find specific nodes by typing the provider name (e.g., 'Stripe', 'Claude', 'Slack') or category. Step 3: Once you've found the desired node, you have two options to add it: Option A - Click and drag the node from the palette onto the canvas, then release at your desired position. Option B - Double-click the node in the palette to add it automatically at the center of your current view. Step 4: After adding, the node will appear with its default configuration. Click on the node to select it and open the configuration panel on the right side. Step 5: Configure the required fields marked with red asterisks. These typically include API keys, endpoint URLs, or essential parameters. Optional fields can be left at their default values. Step 6: Click 'Save Configuration' or press Enter to confirm your changes. The node is now ready to be connected. Pro tips: Use Ctrl+D (Cmd+D on Mac) to duplicate an already configured node. Nodes can be repositioned anytime by clicking and dragging them. Use the mini-map in the bottom-right corner to navigate large workflows quickly. Zoom in/out with mouse wheel or trackpad pinch gesture.",
    "metadata": {
      "action": "add",
      "category": "node",
      "difficulty": "beginner"
    }
  },
  {
    "id": "instruction_delete_node",
    "text": "How to delete a node from your workflow. There are three methods available: Method 1 - Keyboard Shortcut: Select the node by clicking it once (you'll see a highlight border), then press the Delete key or Backspace. Method 2 - Context Menu: Right-click on the node you want to delete. A context menu will appear with various options. Select 'Delete Node' from the menu. Method 3 - Toolbar: Select the node by clicking it, then click the trash icon in the top toolbar. Important warnings before deleting: Deleting a node will automatically remove all connections (edges) attached to it, both incoming and outgoing. If the node is part of a critical workflow path, downstream nodes may become disconnected and won't execute. Consider disconnecting edges first if you want to preserve the overall workflow structure for reconnection later. Undo deletion: Press Ctrl+Z (Cmd+Z on Mac) immediately after deletion to restore the node along with all its connections. The undo history stores the last 50 actions, so you can undo multiple deletions. Bulk delete: Hold Shift and click multiple nodes to select them (they'll all highlight), then delete all at once using any of the methods above. Special restriction: START nodes cannot be deleted if they are the only entry point to your workflow. You must add another trigger node first before deleting the original START node.",
    "metadata": {
      "action": "delete",
      "category": "node",
      "difficulty": "beginner"
    }
  },
  {
    "id": "instruction_connect_nodes",
    "text": "How to connect nodes in your workflow to create data flow between them. Understanding connections: Connections (also called edges) are the lines that pass data from one node's output to another node's input. Think of them as pipes that data flows through. Output ports (small circles) are located on the right side of nodes - these send data out. Input ports are on the left side of nodes - these receive data in. Creating a connection step by step: Step 1: Hover your mouse over an output port on the source node (right side). The port will highlight to indicate it's active. Step 2: Click and hold on the output port. You'll see a connection line start to follow your mouse cursor. Step 3: While holding the mouse button, drag the line to an input port on the target node (left side). Compatible ports will highlight green when you hover over them, indicating the data types match. Incompatible ports will show red or won't highlight, meaning the data types don't match and the connection would fail. Step 4: Release the mouse button over a compatible input port to create the connection. The system will automatically validate that the connection is valid. Data type compatibility rules: String outputs can connect to string inputs directly - perfect match. Number outputs can connect to both number inputs (exact match) and string inputs (automatic conversion to text). Boolean outputs can connect to boolean inputs or string inputs (converted to 'true'/'false'). Object/JSON outputs can have specific fields mapped to primitive inputs using the Field Mapper (double-click the connection line after creating it). Array outputs require a loop node to process each item, or can be mapped directly to array inputs. Any output can connect to DIALOGUE node inputs for display. Best practices for connecting nodes: Keep data flow moving from left to right across your canvas for better readability. Avoid crossing connection lines when possible - reorganize nodes if you see too many intersections. Use the connection color coding to identify data types at a glance (string=blue, number=orange, json=purple). Label complex connections by double-clicking the edge and adding a note describing what data is flowing. Always connect a START node output to begin your workflow - workflows won't execute without a starting point. Common connection mistakes to avoid: Don't connect incompatible types without a transform node in between. Don't create circular dependencies (Node A → Node B → Node A) as this causes infinite loops. Don't forget to connect all required input ports - nodes with missing required inputs will fail during execution.",
    "metadata": {
      "action": "connect",
      "category": "edge",
      "difficulty": "beginner"
    }
  },
  {
    "id": "instruction_disconnect_nodes",
    "text": "How to remove connections between nodes. Method 1 - Select and Delete: Click on the connection line between two nodes to select it (it will highlight). Press Delete or Backspace to remove it. Method 2 - Click the X: Hover over a connection line. A small X button will appear in the middle of the line. Click it to delete the connection. Method 3 - Drag from port: Click and hold on a connected output port, drag away from the input, and release in empty space. This disconnects without creating a new connection. When to disconnect: Before deleting nodes to preserve workflow structure. When reconfiguring data flow. When debugging - temporarily disconnect to isolate issues. To change which nodes are connected. Best practices: Disconnecting doesn't affect the nodes themselves, only the data flow. Undo with Ctrl+Z if you disconnect by accident. Multiple connections to the same input port will automatically remove the previous connection (inputs can only have one source).",
    "metadata": {
      "action": "disconnect",
      "category": "edge",
      "difficulty": "beginner"
    }
  },
  {
    "id": "instruction_configure_node",
    "text": "How to configure a node's settings and parameters. Step 1: Click on the node you want to configure. The configuration panel will appear on the right side of the screen. Step 2: The panel shows several sections: Basic Info - node name, type, and description. Input Fields - parameters required by this node (API keys, URLs, text inputs). Output Fields - data that this node will produce (usually auto-generated based on API response). Advanced Settings - optional parameters that are hidden by default (click 'Show Advanced' to reveal). Step 3: Fill in required fields first - these are marked with red asterisks and cannot be left empty. Common required fields: API Keys - obtain from the service provider's dashboard (Stripe, OpenAI, etc.). Store securely - NEVER hardcode keys directly. Use the credential manager instead. URLs - full endpoint URLs or webhook URLs depending on the API. Input data - default values or placeholder text that will be replaced by connected node outputs. Step 4: Configure optional fields based on your needs: Timeout settings - how long to wait for API response before failing (default 30 seconds). Retry logic - whether to retry failed requests and how many times. Headers - custom HTTP headers for authentication or content type. Query parameters - additional URL parameters for API calls. Step 5: Test configuration by clicking 'Test Node' button if available. This makes a real API call with your settings to verify they work. Check the response in the panel to ensure you're getting expected data. Step 6: Save your configuration by clicking 'Save' or pressing Enter. The node is now ready to use in your workflow. Advanced configuration tips: Use variables/placeholders in input fields that will be populated by upstream nodes at runtime. Example: {customer_email} will be replaced by the actual value from a connected output. Enable 'Transform Response' to modify API responses before passing to next node. Set 'Error Handling' to define what happens if this node fails (stop workflow, continue, or retry). Use 'Conditional Execution' to only run this node if certain conditions are met. Common configuration mistakes: Forgetting to save after making changes. Using test API keys in production workflows. Not providing default values for optional fields that might be empty at runtime. Hardcoding sensitive credentials instead of using the credential manager.",
    "metadata": {
      "action": "configure",
      "category": "node",
      "difficulty": "intermediate"
    }
  },
  {
    "id": "instruction_execute_workflow",
    "text": "How to run and test your workflow. Step 1: Ensure your workflow has a START node. This is the entry point where execution begins. Every workflow must have at least one START node. Step 2: Verify all nodes are properly connected. Check that there are no disconnected nodes (unless they're intentionally unused). Ensure all required input ports are connected or have default values. Look for any error indicators (red exclamation marks) on nodes. Step 3: Click the 'Run' button in the top toolbar. The button looks like a play icon ▶. Alternatively, press Ctrl+Enter (Cmd+Enter on Mac) to start execution. Step 4: Watch the execution progress in real-time. The currently executing node will be highlighted with a pulsing animation. Connections will light up as data flows through them. You'll see a progress indicator showing which step the workflow is on. Step 5: Monitor the Execution Log panel at the bottom of the screen. This shows detailed information about each node as it executes: Timestamp of execution. Node name and type. Success or failure status. Output data produced. Error messages if something goes wrong. Step 6: Review the results when execution completes. Successfully executed nodes show green checkmarks. Failed nodes show red X icons. Click on any node to see its output data in the details panel. DIALOGUE nodes will show popup messages during execution. Step 7: Handle execution errors: Read the error message in the Execution Log to understand what went wrong. Common errors include: Invalid API keys or authentication failures. Network timeouts or connectivity issues. Invalid input data format. Rate limiting from API providers. Missing required fields. Fix the error by reconfiguring the affected node or adjusting connections. Run the workflow again after making corrections. Testing best practices: Start with a simple workflow before building complex multi-step processes. Test each node individually first by creating a minimal workflow: START → Single Node → DIALOGUE. Use test/sandbox API credentials during development, not production keys. Enable 'Debug Mode' to see detailed logs and variable values at each step. Test error scenarios - intentionally provide bad input to ensure error handling works. Use DIALOGUE nodes strategically to inspect intermediate data at various points. Save working versions before making major changes - use workflow versioning. Execution modes: Normal Run - executes the entire workflow from start to finish. Step-by-Step - pauses after each node for manual inspection (click 'Next' to continue). Debug Run - provides detailed logging and allows breakpoints. Dry Run - simulates execution without actually calling APIs or modifying data (useful for testing logic). Performance tips: Workflows with many nodes may take time to complete - be patient. Use conditional logic to skip unnecessary nodes and speed up execution. Consider parallel execution where possible - nodes without dependencies can run simultaneously. Monitor API rate limits to avoid failures in long-running workflows. Debugging failed workflows: Check the Execution Log for the first node that failed - errors often cascade from there. Verify API credentials are valid and have not expired. Ensure input data format matches what the API expects. Test API endpoints directly using Postman or curl to isolate issues. Check network connectivity if you see timeout errors. Review API provider status pages for outages or maintenance.",
    "metadata": {
      "action": "execute",
      "category": "workflow",
      "difficulty": "intermediate"
    }
  },
  {
    "id": "instruction_field_mapping",
    "text": "How to map fields between nodes with different schemas. When to use Field Mapping: Use field mapping when connecting two nodes where the output field names don't match the input field names. Example: Node A outputs 'user_email' but Node B expects 'recipient' - you need to map user_email → recipient. Another example: An API returns a complex JSON object but you only need specific nested fields. Opening the Field Mapper: Method 1: Click on the connection line between two nodes, then click the 'Edit Mapping' button in the popup tooltip. Method 2: Double-click directly on the connection line to open the Field Mapper immediately. The Field Mapper interface has two main columns: Left column: Shows all output fields from the source node with their data types (string, number, boolean, object, array). Right column: Shows all input fields of the target node with requirements (required fields marked with *). Using the Field Mapper: Step 1: Drag from a source field (left) to a target field (right) to create a mapping. Valid mappings show a green line connecting the fields. Invalid type mismatches show a red line with a warning - hover for details. Step 2: Review auto-suggested mappings. The system attempts to match fields by name similarity and shows them with dotted lines. Click 'Accept Suggestion' to apply a suggested mapping, or ignore it to map manually. Step 3: Add transformations if data types don't match exactly. Click the transformation icon (gear) on a mapping line. Available transformations include: to_string - convert any type to string representation. to_number - parse string to number (e.g., '123' → 123). to_boolean - convert to true/false (e.g., 'yes' → true). format_date - convert date format (e.g., ISO 8601 to MM/DD/YYYY). parse_json - parse JSON string into object. stringify_json - convert object to JSON string. uppercase / lowercase - text transformations. trim - remove whitespace. extract_field - pull specific field from nested object. Step 4: Use AI Auto-Map feature for complex mappings. Click 'Auto-Map with AI' button at the top of the Field Mapper. The AI analyzes field names, types, and common API patterns to suggest mappings. Review AI suggestions carefully before accepting - they're intelligent but not perfect. AI can map semantic similarities like 'customer_email' → 'recipient_address', 'created_timestamp' → 'event_date'. Step 5: Handle nested objects and arrays. For object outputs, you'll see expandable tree view - click to reveal nested fields. Map nested fields using dot notation: object.nested.field → target. For array outputs, you can: Map entire array to array input (direct). Use 'Extract first item' transformation to get arrays[0] → single value. Connect to a Loop node to process each array item individually. Step 6: Validate and save your mappings. The Field Mapper shows validation status: Green checkmark = all required fields mapped correctly. Yellow warning = optional fields unmapped (usually okay). Red error = required fields missing or type mismatch. Click 'Save Mapping' to apply changes and close the editor. Best practices for field mapping: Map all required fields first (marked with *), then handle optional fields. Test your mappings by running the workflow and inspecting output data. Use descriptive transformation chains for complex conversions - they're easier to debug. Document unusual mappings with comments (click comment icon on mapping line). For APIs you use frequently, save mapping templates for reuse. Common mapping mistakes: Forgetting to save after creating mappings - changes won't apply. Mapping incompatible types without transformations (e.g., object → string without stringify). Not handling null/undefined values - add default value transformations as fallback. Circular mappings that reference themselves. Over-complicating maps - sometimes it's better to use a Transform node for very complex logic.",
    "metadata": {
      "action": "map",
      "category": "edge",
      "difficulty": "advanced"
    }
  },
  {
    "id": "instruction_error_handling",
    "text": "How to handle errors and failures in workflows. Why error handling is critical: APIs can fail due to network issues, invalid credentials, rate limits, or bad input data. Without error handling, your entire workflow stops at the first failure. Good error handling makes workflows robust and production-ready. Built-in error handling options: Each node has an 'Error Handling' section in its configuration panel: Stop on Error (default) - workflow stops immediately if this node fails. Continue on Error - workflow proceeds even if node fails, outputs empty/null values. Retry on Error - automatically retry failed requests with exponential backoff. Configure: retry count (default 3), delay between retries (default 1s, then 2s, then 4s). Fallback Value - if node fails, use this predefined value as output instead. Setting up conditional error handling: Step 1: Add a Conditional node after any node that might fail. Step 2: Configure the Conditional to check for error states: Condition: status !== 'success' or error_message !== null. Step 3: Create two paths from Conditional: True path (error occurred) - connect to error handling logic. False path (success) - connect to normal workflow continuation. Error handling node patterns: Pattern 1 - Log and Alert: Connect error path to DIALOGUE node showing error message. Connect to Slack/Discord webhook to notify team. Connect to database logging node to record error for analysis. Pattern 2 - Retry with Delay: Connect error path to Timer node (wait 5 seconds). Connect Timer back to the original failing node for retry. Limit retries with a counter variable to prevent infinite loops. Pattern 3 - Fallback Alternative: Connect error path to alternative API or data source. Example: Primary API fails → try backup API → use cached data → show default message. Pattern 4 - Graceful Degradation: If optional feature fails, continue without it. Example: Analytics tracking fails → log error → continue user workflow. Best practices for error handling: Always handle errors for critical payment or data modification nodes. Log errors to external service for monitoring (Slack, logging API, database). Include context in error logs: node name, input data, timestamp, user ID. Test error scenarios by intentionally triggering failures during development. Set appropriate timeouts - don't wait forever for slow APIs. Implement circuit breaker pattern for repeatedly failing services (stop trying after N failures). Use retry logic for transient errors (network issues), but not for permanent errors (invalid credentials). Provide user-friendly error messages, not raw API error responses. Common errors and how to handle them: Authentication errors (401, 403): Check API key validity. Verify key hasn't expired or been revoked. Ensure correct authentication header format. Don't retry - fix credentials first. Rate limiting (429): Implement exponential backoff. Add delays between requests. Consider caching results to reduce API calls. Use batch endpoints if available. Timeouts: Increase timeout setting if API is legitimately slow. Check network connectivity. Consider using async/webhook patterns for long-running operations. Invalid input data (400): Validate input data before sending to API. Add Transform node to format data correctly. Check API documentation for required format. Log the exact data that caused error for debugging. Server errors (500-599): Retry with exponential backoff. Contact API provider if persistent. Have fallback option ready. Log for investigation. Advanced error handling: Use global error handler to catch any unhandled errors in workflow. Implement dead letter queue pattern - failed items go to separate queue for manual review. Send error metrics to monitoring dashboard (track error rates, types, affected users). Create error recovery workflows that run periodically to retry failed operations.",
    "metadata": {
      "action": "handle",
      "category": "error",
      "difficulty": "advanced"
    }
  },
  {
    "id": "instruction_workflow_best_practices",
    "text": "Best practices for building robust, maintainable, and efficient API workflows. 1. Planning and Design: Before building, sketch your workflow on paper or whiteboard. Identify all required APIs and data transformations. Define success criteria - what should the workflow accomplish? Plan for error scenarios, not just happy path. Document assumptions about data formats and API behavior. 2. Error Handling: Always add error handler nodes after API calls - APIs can and will fail. Use try-catch patterns with conditional nodes to route failures appropriately. Log errors to external monitoring service (Slack, logging API) for debugging. Implement retry logic for transient failures like network timeouts or rate limits. Add fallback options for critical operations. Never let a workflow fail silently - always notify someone or log the error. Test error scenarios - intentionally provide bad data to verify error handling works. 3. Data Validation: Add validation nodes before critical API calls to check data quality. Verify required fields are present and non-empty. Check data types match expected format (string, number, etc.). Validate ranges and constraints (e.g., price > 0, email format). Use schema validation to catch issues early before expensive API calls. Provide meaningful error messages for invalid data to aid debugging. Sanitize user input to prevent injection attacks or malformed data. 4. Performance Optimization: Identify independent branches that can execute in parallel to speed up workflows. Use caching nodes for repeated API calls with same parameters - don't call same API twice. Implement pagination handling for large datasets to avoid memory issues and timeouts. Set appropriate timeouts for each node - don't use same timeout for quick vs slow APIs. Minimize data transformation steps - combine multiple transforms into one when possible. Use batch API endpoints when available instead of individual calls in loops. Monitor execution time and optimize bottlenecks. 5. Security Best Practices: NEVER hardcode API keys or credentials directly in node configurations. Use the credential manager to store and retrieve secrets securely. Encrypt sensitive data in transit and at rest. Implement rate limiting to prevent API abuse and unexpected costs. Audit log all credential access and API calls for security monitoring. Use least privilege principle - only request necessary API permissions. Rotate API keys regularly and have process to revoke compromised keys. Validate SSL certificates when making external API calls. 6. Testing and Debugging: Use mock/sandbox API credentials during development, not production keys. Test each node individually before testing full workflow - build incrementally. Create test cases for success paths AND error conditions - don't just test happy path. Use DIALOGUE nodes strategically to inspect intermediate data at various workflow points. Enable debug mode to see detailed logs and variable values at each execution step. Test with realistic data volumes, not just small test datasets. Verify workflow behaves correctly under load and with concurrent executions. Document test scenarios and expected outcomes for regression testing. 7. Documentation and Maintainability: Name nodes descriptively - use 'Process Stripe Payment' not 'API Call 1'. Add comment nodes to explain complex logic or business rules. Document expected inputs and outputs for the entire workflow. Include examples of valid input data format. Keep a changelog documenting workflow modifications and reasons. Version your workflows - save copies before making major changes. Use consistent naming conventions across your organization's workflows. Create reusable sub-workflows for common patterns (authentication, error handling). 8. Monitoring and Observability: Implement logging at key points in workflow to track execution progress. Send execution metrics to monitoring dashboard (success rate, execution time, error types). Set up alerts for abnormal behavior (high error rates, slow execution, failed authentications). Track API usage and costs to avoid unexpected bills. Monitor rate limit consumption to prevent service disruptions. Log both successes and failures for comprehensive audit trail. Include correlation IDs to trace requests across distributed systems. 9. Cost Management: Understand pricing models for all APIs used (per-call, bandwidth, compute time). Use preview/dry-run mode to estimate costs before production deployment. Implement cost limits and alerts for expensive operations. Cache expensive API calls when data doesn't change frequently. Use cheaper alternatives for non-critical operations. Monitor usage dashboards provided by API providers. 10. Workflow Organization: Group related nodes visually for better readability. Use consistent left-to-right flow for main execution path. Place error handling nodes below main path. Use color coding or labels to categorize node types. Minimize crossing connection lines - reorganize nodes if workflow looks messy. Use sub-workflows to break down complex logic into manageable pieces. Keep workflows focused on single purpose - avoid monolithic workflows doing everything.",
    "metadata": {
      "action": "build",
      "category": "workflow",
      "difficulty": "advanced"
    }
  },
  {
    "id": "instruction_save_load_workflow",
    "text": "How to save and load your workflows. Workflows are auto-saved in NodeLink: Every change you make is automatically saved to your MongoDB database. You don't need to manually save - changes persist immediately. The save indicator in top-right shows last save time. If you see 'Saving...' it means changes are being persisted. Workflow versioning: Each project can have multiple workflows. Create new workflow by clicking 'New Workflow' in project view. Access existing workflows from the project dashboard. Workflows are organized by project - each project can contain many workflows. Exporting workflows: Click the export button (download icon) in the toolbar. Choose export format: JSON (full workflow data including node positions, connections, configurations), or Template (sharable format without sensitive credentials). Save the exported file to your local machine. Exported workflows can be shared with team members or backed up for disaster recovery. Importing workflows: Click import button (upload icon) in toolbar. Select a previously exported workflow JSON file. The workflow will load with all nodes, connections, and configurations restored. Note: Credentials are NOT included in exports for security - you'll need to reconfigure API keys after import. Loading from template: When creating a new project, select 'Use Template' option. Choose from pre-built templates showcasing common API integration patterns. Template workflows load fully configured with example nodes and connections. You only need to add your own API credentials to make them functional. Best practices: Regularly export critical workflows as backups. Use version control for exported workflow JSON files (store in Git repository). Create templates for frequently used patterns to save time. Test imported workflows before using in production. Document any manual configuration needed after import.",
    "metadata": {
      "action": "save",
      "category": "workflow",
      "difficulty": "beginner"
    }
  }
]
